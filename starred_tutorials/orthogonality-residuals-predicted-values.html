<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Matrix Proof: Orthogonality of Residuals and Centered Predicted Values</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1, h2, h3 {
            text-align: center;
        }
        .section {
            margin: 1em 0;
            padding: 20px;
            background-color: #f0f0f0;
            border-radius: 10px;
        }
        .equation {
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 1em 0;
        }
    </style>
</head>
<body>
    <h1>Matrix Proof: Orthogonality of Residuals and Centered Predicted Values</h1>

    <div class="section">
        <h2>1. Setup and Definitions</h2>
        <p>Let's define our matrices and vectors:</p>
        <ul>
            <li>y: n × 1 vector of observed values</li>
            <li>ŷ: n × 1 vector of predicted values</li>
            <li>e: n × 1 vector of residuals (e = y - ŷ)</li>
            <li>1: n × 1 vector of ones</li>
            <li>I: n × n identity matrix</li>
            <li>H: n × n hat matrix (H = X(X'X)⁻¹X')</li>
            <li>ȳ: scalar, mean of y</li>
        </ul>
    </div>

    <div class="section">
        <h2>2. Theorem to Prove</h2>
        <p>We want to prove:</p>
        <div class="equation">
            \[
            \sum_{i=1}^n (y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) = 0
            \]
        </div>
        <p>In matrix notation, this is equivalent to:</p>
        <div class="equation">
            \[
            \mathbf{e}'(\hat{\mathbf{y}} - \bar{y}\mathbf{1}) = 0
            \]
        </div>
    </div>

    <div class="section">
        <h2>3. Proof</h2>
        
        <h3>Step 1: Express e and ŷ in terms of y and H</h3>
        <div class="equation">
            \[
            \begin{aligned}
            \mathbf{e} &= \mathbf{y} - \hat{\mathbf{y}} = (\mathbf{I} - \mathbf{H})\mathbf{y} \\
            \hat{\mathbf{y}} &= \mathbf{H}\mathbf{y}
            \end{aligned}
            \]
        </div>

        <h3>Step 2: Express ȳ in terms of y</h3>
        <div class="equation">
            \[
            \bar{y} = \frac{1}{n}\mathbf{1}'\mathbf{y}
            \]
        </div>

        <h3>Step 3: Substitute into the expression we want to prove</h3>
        <div class="equation">
            \[
            \begin{aligned}
            \mathbf{e}'(\hat{\mathbf{y}} - \bar{y}\mathbf{1}) &= [(\mathbf{I} - \mathbf{H})\mathbf{y}]'[\mathbf{H}\mathbf{y} - \frac{1}{n}\mathbf{1}'\mathbf{y}\mathbf{1}] \\
            &= \mathbf{y}'(\mathbf{I} - \mathbf{H})[\mathbf{H} - \frac{1}{n}\mathbf{1}\mathbf{1}']\mathbf{y}
            \end{aligned}
            \]
        </div>

        <h3>Step 4: Use properties of H and 1</h3>
        <p>Key properties:</p>
        <ul>
            <li>H is idempotent: H² = H</li>
            <li>H is symmetric: H' = H</li>
            <li>H1 = 1 (the regression includes an intercept)</li>
            <li>(I - H)1 = 0 (consequence of H1 = 1)</li>
        </ul>
        
        <h3>Step 5: Simplify the expression</h3>
        <div class="equation">
            \[
            \begin{aligned}
            \mathbf{y}'(\mathbf{I} - \mathbf{H})[\mathbf{H} - \frac{1}{n}\mathbf{1}\mathbf{1}']\mathbf{y} &= \mathbf{y}'[\mathbf{H} - \mathbf{H}^2 - \frac{1}{n}(\mathbf{I} - \mathbf{H})\mathbf{1}\mathbf{1}']\mathbf{y} \\
            &= \mathbf{y}'[\mathbf{H} - \mathbf{H} - \frac{1}{n}(\mathbf{0})\mathbf{1}']\mathbf{y} \\
            &= \mathbf{y}'\mathbf{0}\mathbf{y} = 0
            \end{aligned}
            \]
        </div>
    </div>

    <div class="section">
        <h2>4. Interpretation and Implications</h2>
        <ul>
            <li><strong>Geometric Interpretation:</strong> The residuals are orthogonal to the centered predicted values.</li>
            <li><strong>Regression Through the Mean:</strong> This property ensures that the regression line passes through the point (x̄, ȳ).</li>
            <li><strong>Partitioning of Sums of Squares:</strong> This orthogonality allows for the clean decomposition of total sum of squares into explained and residual sum of squares.</li>
            <li><strong>Efficiency of OLS:</strong> It's a key property in proving that OLS estimators are BLUE (Best Linear Unbiased Estimators).</li>
        </ul>
    </div>

    <div class="section">
        <h2>5. Conclusion</h2>
        <p>We have proven that Σ(yᵢ - ŷᵢ)(ŷᵢ - ȳ) = 0 using matrix notation. This proof demonstrates the orthogonality of residuals to centered predicted values, which is a fundamental property in regression analysis. It underlies many important results in linear regression theory and provides insights into the nature of least squares estimation.</p>
    </div>
</body>
</html>
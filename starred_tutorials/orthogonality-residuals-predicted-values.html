<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proof: Orthogonality of Residuals and Predicted Values in Linear Regression</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1, h2, h3 {
            text-align: center;
        }
        .section {
            margin: 1em 0;
            padding: 20px;
            background-color: #f0f0f0;
            border-radius: 10px;
        }
        .equation {
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 1em 0;
        }
    </style>
</head>
<body>
    <h1>Proof: Orthogonality of Residuals and Predicted Values in Linear Regression</h1>

    <div class="section">
        <h2>1. Introduction</h2>
        <p>We will prove that in linear regression, the residuals (y - ŷ) are orthogonal to the predicted values (ŷ - ȳ). Mathematically, we need to show that:</p>
        <div class="equation">
            \[
            \sum (y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) = 0
            \]
        </div>
        <p>This property is crucial in regression analysis and underlies many important results.</p>
    </div>

    <div class="section">
        <h2>2. Setup and Definitions</h2>
        <ul>
            <li>y_i: observed values</li>
            <li>ŷ_i: predicted values</li>
            <li>ȳ: mean of observed values</li>
            <li>x_i: predictor values</li>
            <li>x̄: mean of predictor values</li>
            <li>β̂₀, β̂₁: estimated regression coefficients</li>
        </ul>
        <p>In simple linear regression, we have:</p>
        <div class="equation">
            \[
            \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
            \]
        </div>
        <p>And the line passes through (x̄, ȳ), so:</p>
        <div class="equation">
            \[
            \bar{y} = \hat{\beta}_0 + \hat{\beta}_1 \bar{x}
            \]
        </div>
    </div>

    <div class="section">
        <h2>3. Proof</h2>
        
        <h3>Step 1: Express the sum in terms of regression coefficients</h3>
        <div class="equation">
            \[
            \begin{aligned}
            \sum (y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) &= \sum (y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))((\hat{\beta}_0 + \hat{\beta}_1 x_i) - (\hat{\beta}_0 + \hat{\beta}_1 \bar{x})) \\
            &= \sum (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)(\hat{\beta}_1 x_i - \hat{\beta}_1 \bar{x}) \\
            &= \hat{\beta}_1 \sum (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)(x_i - \bar{x})
            \end{aligned}
            \]
        </div>

        <h3>Step 2: Use the normal equations</h3>
        <p>Recall that in OLS, the normal equations give us:</p>
        <div class="equation">
            \[
            \sum (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
            \]
        </div>
        <div class="equation">
            \[
            \sum (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)x_i = 0
            \]
        </div>

        <h3>Step 3: Expand the sum</h3>
        <div class="equation">
            \[
            \begin{aligned}
            \hat{\beta}_1 \sum (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)(x_i - \bar{x}) &= \hat{\beta}_1 [\sum (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)x_i - \bar{x}\sum (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)] \\
            &= \hat{\beta}_1 [0 - \bar{x} \cdot 0] \quad \text{(using the normal equations)} \\
            &= 0
            \end{aligned}
            \]
        </div>
    </div>

    <div class="section">
        <h2>4. Geometric Interpretation</h2>
        <p>Geometrically, this result means that the vector of residuals is perpendicular to the vector of predicted values (when both are centered around their means). This can be visualized as follows:</p>
        <div class="equation">
            <svg width="300" height="300" viewBox="-10 -10 320 320">
                <defs>
                    <marker id="arrowhead" markerWidth="10" markerHeight="7" 
                    refX="0" refY="3.5" orient="auto">
                        <polygon points="0 0, 10 3.5, 0 7" />
                    </marker>
                </defs>
                <line x1="0" y1="300" x2="300" y2="0" stroke="blue" stroke-width="2"/>
                <line x1="150" y1="150" x2="250" y2="50" stroke="red" stroke-width="2" marker-end="url(#arrowhead)"/>
                <line x1="150" y1="150" x2="50" y2="250" stroke="green" stroke-width="2" marker-end="url(#arrowhead)"/>
                <text x="260" y="40" fill="red">Predicted Values</text>
                <text x="40" y="270" fill="green">Residuals</text>
                <text x="280" y="280" fill="blue">Regression Line</text>
            </svg>
        </div>
    </div>

    <div class="section">
        <h2>5. Implications and Importance</h2>
        <ul>
            <li><strong>Decomposition of Variance:</strong> This property allows for the clean decomposition of total sum of squares into explained and residual sum of squares.</li>
            <li><strong>Efficiency of OLS:</strong> It ensures that OLS estimates have minimum variance among linear unbiased estimators (Gauss-Markov theorem).</li>
            <li><strong>Model Diagnostics:</strong> It's a key assumption in many diagnostic procedures, such as residual plots.</li>
            <li><strong>Intuitive Understanding:</strong> It means that the residuals contain no linear information about the predictors that could improve the fit.</li>
        </ul>
    </div>

    <div class="section">
        <h2>6. Conclusion</h2>
        <p>We have proven that Σ(y - ŷ)(ŷ - ȳ) = 0, demonstrating the orthogonality of residuals and predicted values in linear regression. This fundamental property underpins many theoretical results in regression analysis and provides insights into the nature of least squares estimation.</p>
    </div>
</body>
</html>
